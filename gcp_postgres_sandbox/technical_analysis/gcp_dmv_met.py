# -*- coding: utf-8 -*-

"""
dmv_met

Automatically generated by Colab.
Original file is located at-
    https://colab.research.google.com/drive/1C5hRrp26kdOspV8U9L-G8YE2pQNTm2Mr

#Extract Base Data
"""

# pip install mysql-connector-python

# @title LIBRARY
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sqlalchemy import create_engine
import pandas as pd
warnings.filterwarnings('ignore')

import time
start_time = time.time()

# @title  GCP/Cloud DB connect
from sqlalchemy import create_engine
import pandas as pd

# Connection parameters
db_host = "34.55.195.199"         # Public IP of your PostgreSQL instance on GCP
db_name = "dbcp"                  # Database name
db_name_bt = "cp_backtest"                  # Database name
db_user = "yogass09"              # Database username
db_password = "jaimaakamakhya"     # Database password
db_port = 5432                    # PostgreSQL port

# Create a SQLAlchemy engine for PostgreSQL
gcp_engine = create_engine(f'postgresql+pg8000://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')

# Create a SQLAlchemy engine for PostgreSQL
gcp_engine_bt = create_engine(f'postgresql+pg8000://{db_user}:{db_password}@{db_host}:{db_port}/{db_name_bt}')

# @title SQL Query Connection to AWS for Data Listing

# Executing the query and fetching the results directly into a pandas DataFrame
with gcp_engine.connect() as connection:
    query = 'SELECT * FROM "1K_coins_ohlcv"'  # Enclose only the table name in double quotes
    all_coins_ohlcv_filtered = pd.read_sql_query(query, connection)

    query = "SELECT * FROM crypto_listings_latest_1000"
    top_1000_cmc_rank= pd.read_sql_query(query, connection)


# @title  Enhancing Function Definition Through Grouping and Indexing Techniques
df=all_coins_ohlcv_filtered
df.set_index('symbol', inplace=True)
# Ensure the timestamp column is in datetime format
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Sort the DataFrame by 'slug' and 'timestamp' columns
df.sort_values(by=['slug', 'timestamp'], inplace=True)

# Perform time-series calculations within each group (each cryptocurrency)
grouped = df.groupby('slug')

"""# METRICS"""

# @title  Enhancing Function Definition Through Grouping and Indexing Techniques

df=all_coins_ohlcv_filtered
# Ensure the timestamp column is in datetime format
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Sort the DataFrame by 'slug' and 'timestamp' columns
df.sort_values(by=['slug', 'timestamp'], inplace=True)

# Perform time-series calculations within each group (each cryptocurrency)
grouped = df.groupby('slug')

# @title Calculate Daily Percentage Change & Calculate Cumulative Returns

# Calculate percentage change for each cryptocurrency
df['m_pct_1d'] = grouped['close'].pct_change()

# Calculate cumulative returns for each cryptocurrency
df['d_pct_cum_ret'] = (1 + df['m_pct_1d']).groupby(df['slug']).cumprod() - 1

# prompt: df in this like above codes we need to find ath and atl which is alltime high and all time low

# Calculate all-time high (ATH) for each cryptocurrency
df['v_met_ath'] = grouped['high'].cummax()

# Calculate all-time low (ATL) for each cryptocurrency
df['v_met_atl'] = grouped['low'].cummin()

df.set_index('timestamp', inplace=True)

# Calculate the date when ATH was reached
df['ath_date'] = df.groupby('slug')['high'].transform(lambda x: x.idxmax())

# Calculate the date when ATL was reached
df['atl_date'] = df.groupby('slug')['low'].transform(lambda x: x.idxmin())
df.reset_index(inplace=True)

# Convert to datetime format
df['ath_date'] = pd.to_datetime(df['ath_date'])
df['atl_date'] = pd.to_datetime(df['atl_date'])

df.head()

# Step 3: Calculate the number of days since ATH and ATL
current_date = pd.Timestamp.now().tz_localize(None)  # Ensure current_date is tz-naive
df['ath_date'] = df['ath_date'].dt.tz_localize(None)  # Ensure ath_date is tz-naive
df['atl_date'] = df['atl_date'].dt.tz_localize(None)  # Ensure atl_date is tz-naive

df['d_met_ath_days'] = (current_date - df['ath_date']).dt.days
df['d_met_atl_days'] = (current_date - df['atl_date']).dt.days

# Convert days to weeks and months
df['d_met_ath_week'] = df['d_met_ath_days'] // 7
df['d_met_ath_month'] = df['d_met_ath_days'] // 30

df['d_met_atl_week'] = df['d_met_atl_days'] // 7
df['d_met_atl_month'] = df['d_met_atl_days'] // 30

df.head()

# @title Keeping Only the Latest Data for Each Cryptocurrency Slug
# prompt: in df .. keep only latest or max of timestamp for every slug #... by that i mean .. i need only all slugs one row data which is ;atest

# Group by 'slug' and get the row with the maximum timestamp
met_df1 = df.loc[df.groupby('slug')['timestamp'].idxmax()]

# Perform the inner join
met_df1 = pd.merge(met_df1, top_1000_cmc_rank[['slug', 'date_added', 'last_updated']], on='slug', how='inner')

# @title Analysis for CoinAge and Mcap  Metrics

df=met_df1
# Convert 'date_added' and 'last_updated' columns to datetime without time zone information
df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce').dt.tz_localize(None)
df['last_updated'] = pd.to_datetime(df['last_updated'], errors='coerce').dt.tz_localize(None)

# Ensure there are no NaT values in 'date_added' or 'last_updated'
df = df.dropna(subset=['date_added', 'last_updated'])

# Step 1: Calculate CoinAge in Days, Months, and Years
current_date = pd.Timestamp.now().normalize()  # Normalize to remove time part if present
df['d_met_coin_age_d'] = (current_date - df['date_added']).dt.days
df['d_met_coin_age_m'] = df['d_met_coin_age_d'] // 30  # Approximate months
df['d_met_coin_age_y'] = df['d_met_coin_age_d'] // 365  # Approximate years

# Step 2: Categorize Market Capitalization
def categorize_market_cap(market_cap):
    if market_cap >= 1e12:
        return '1T-100B'
    elif market_cap >= 1e11:
        return '100B-10B'
    elif market_cap >= 1e10:
        return '10B-1B'
    elif market_cap >= 1e9:
        return '1B-100M'
    elif market_cap >= 1e7:
        return '100M-1M'
    else:
        return 'Under1M'

df['m_cap_cat'] = df['market_cap'].apply(categorize_market_cap)

metrics=df

# prompt: metrics can we make sure we dont have any slug duplicates in this

# Check for duplicates in the 'slug' column
duplicate_slugs = metrics[metrics.duplicated(subset=['slug'], keep=False)]

# If duplicates exist, print them
if not duplicate_slugs.empty:
  print("Duplicate slugs found:")
  print(duplicate_slugs['slug'].unique())

# Remove duplicates, keeping the first occurrence
metrics_no_duplicates = metrics.drop_duplicates(subset=['slug'], keep='first')

# prompt: drop col 4:9 in metrics

metrics = metrics.drop(metrics.columns[4:10], axis=1)
metrics.info()

# @title SQLalchemy to push data to aws db (mysql)

from sqlalchemy import create_engine


# Create a SQLAlchemy engine for PostgreSQL
gcp_engine = create_engine(f'postgresql+pg8000://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')

# Write the DataFrame to a new table in the database
metrics.to_sql('FE_METRICS', con=gcp_engine, if_exists='replace', index=False)

print("Metrics DataFrame uploaded to dbcp database successfully!")

# Create a SQLAlchemy engine for PostgreSQL
gcp_engine_bt = create_engine(f'postgresql+pg8000://{db_user}:{db_password}@{db_host}:{db_port}/{db_name_bt}')

# Write the DataFrame to a new table in the database
metrics.to_sql('FE_METRICS', con=gcp_engine_bt, if_exists='replace', index=False)

print("Metrics DataFrame uploaded to cp_backtest database successfully!")


# @title Metrics Signals
# m_pct_1d signal
metrics['m_pct_1d_signal'] = np.where(metrics['m_pct_1d'] > 0, 1, -1)

# d_pct_cum_ret signal
metrics['d_pct_cum_ret_signal'] = np.where(metrics['d_pct_cum_ret'] > 0, 1, -1)

# _met_ath_month signal
metrics['d_met_ath_month_signal'] = ((100 - metrics['d_met_ath_month']) * 2)/100

# Define the mapping for market cap categories
market_cap_signal = {
    '100M-1M': 0.25,
    '1B-100M': .4,
    'Under1M': 0.1,
    '10B-1B': 0.5,
    '1T-100B': 1,
    '100B-10B': 0.75
}

# Create a new column 'd_market_cap_signal' based on the mapping
metrics['d_market_cap_signal'] = metrics['m_cap_cat'].map(market_cap_signal)


# d_met_coin_age_y signal
metrics['d_met_coin_age_y_signal'] = np.where(
    metrics['d_met_coin_age_y'] < 1, 0,
    np.where(metrics['d_met_coin_age_y'] >= 1, 1 - (1 / metrics['d_met_coin_age_y']), 0)
)

metrics.info()

metrics_signal = metrics.drop(metrics.columns[3:22], axis=1)

metrics_signal.info()

# @title SQLalchemy to push data to aws db (mysql)

from sqlalchemy import create_engine

# Create a SQLAlchemy engine to connect to the MySQL database
#engine = create_engine('mysql+mysqlconnector://yogass09:jaimaakamakhya@dbcp.cry66wamma47.ap-south-1.rds.amazonaws.com:3306/dbcp')

# Write the DataFrame to a new table in the database
metrics_signal.to_sql('FE_METRICS_SIGNAL', con=gcp_engine, if_exists='replace', index=False)

print("FE_METRICS_SIGNAL DataFrame uploaded to dbcp database successfully!")

# Create a SQLAlchemy engine for PostgreSQL
gcp_engine_bt = create_engine(f'postgresql+pg8000://{db_user}:{db_password}@{db_host}:{db_port}/{db_name_bt}')

# Write the DataFrame to a new table in the database
metrics_signal.to_sql('FE_METRICS_SIGNAL', con=gcp_engine_bt, if_exists='replace', index=False)

end_time = time.time()
elapsed_time_seconds = end_time - start_time
elapsed_time_minutes = elapsed_time_seconds / 60

print(f"Cell execution time: {elapsed_time_minutes:.2f} minutes")


gcp_engine.dispose()


"""# end of script

"""




